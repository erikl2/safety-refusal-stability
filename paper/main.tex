\documentclass[11pt]{article}

% Page formatting similar to NeurIPS
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

\title{The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior}

\author{%
  Erik Larsen \\
  \texttt{elarsen.mailbox@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures $\times$ 5 random seeds), we find that \textbf{18--28\% of prompts exhibit decision flips}---the model refuses in some configurations but complies in others---depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability ($\chi^2 = 133.15$, $p < 0.001$), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 91.7\% inter-judge agreement with our primary Llama 70B judge (Cohen's $\kappa = 0.744$). The most stable model (Gemma 3 12B, SSI=0.965) is also the least restrictive (78.5\% refusal rate), suggesting a potential stability-conservatism tradeoff. These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment and that evaluation protocols must account for stochastic variation in model behavior. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4\% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.
\end{abstract}

\section{Introduction}

As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their safety has become paramount. Current safety evaluation methodologies typically assess model responses to harmful prompts through single-shot testing: each prompt is evaluated once, and the model's response is classified as either refusing or complying with the harmful request. This approach implicitly assumes that model responses are deterministic and that a single sample accurately represents the model's safety behavior.

However, modern LLMs employ stochastic sampling during inference, introducing variability through both temperature-controlled randomness and random seed initialization. While this variability is well-documented for general generation tasks, its impact on safety-critical decisions remains underexplored. If safety decisions are unstable across sampling configurations, single-shot evaluations may significantly misrepresent a model's true safety profile---either overestimating safety by catching a safe sample from an unstable prompt, or underestimating it by observing a failure case that rarely occurs.

In this work, we systematically investigate the stability of LLM safety refusal behavior by testing the same harmful prompts across multiple random seeds and temperature settings. We introduce the \textbf{Safety Stability Index (SSI)}, a metric that quantifies how consistently a model makes the same safety decision across different sampling configurations. We evaluate four instruction-tuned models from three families---Llama 3.1 8B Instruct, Qwen 2.5 7B Instruct, Qwen 3 8B, and Gemma 3 12B Instruct---on 876 harmful prompts from the BeaverTails dataset across 20 configurations (4 temperatures $\times$ 5 random seeds), generating 70,080 total responses. To validate our findings and address potential same-family judge bias, we additionally evaluate the newer models using Claude 3.5 Haiku as an external judge.

Our findings reveal significant instability in safety decisions:
\begin{itemize}
    \item \textbf{24.8\% of prompts exhibit decision flips} across sampling configurations on average, ranging from 18--28\% depending on the model
    \item \textbf{Temperature significantly affects stability} ($\chi^2 = 133.15$, $p < 0.001$): Lower temperatures yield more stable decisions (mean SSI = 0.951 at temp 0.0) compared to higher temperatures (mean SSI = 0.896 at temp 1.0)
    \item \textbf{Instability is consistent across model families}: All four models from three families (Llama, Qwen, Gemma) exhibit instability patterns despite different training approaches, suggesting this is a fundamental property of current safety training
    \item \textbf{Borderline prompts exist}: 7.9\% of prompts fall in the highly unstable range (SSI 0.4--0.6), predominantly copyright-related requests where the model is uncertain whether to comply
    \item \textbf{Single-shot evaluation is unreliable}: Evaluating with N=1 sample agrees with ground truth only 92.4\% of the time; N$\geq$3 samples are needed for 95\% reliability
\end{itemize}

These results have important implications for safety evaluation practices. Single-shot testing may give a false sense of security or unnecessarily penalize models depending on which random seed is used. We argue that safety benchmarks should report stability metrics alongside accuracy, and that deployment configurations should carefully consider temperature settings to maximize both safety and consistency.

\section{Related Work}

\textbf{Safety Evaluation Benchmarks.} Safety evaluation of large language models has become a critical research area, with several benchmarks developed to assess model behavior on harmful prompts. BeaverTails~\cite{ji2023beavertails} provides a human-preference dataset for safety alignment covering diverse harm categories. HarmBench~\cite{mazeika2024harmbench} offers a standardized framework for automated red teaming and robust refusal evaluation. AdvBench~\cite{zou2023advbench} focuses on adversarial attacks against aligned models. These benchmarks typically rely on single-shot testing---each prompt is evaluated once with a fixed random seed and temperature---implicitly assuming that model responses are deterministic or that a single sample adequately represents the model's safety behavior.

\textbf{LLM Calibration and Consistency.} Recent work has investigated whether language models can accurately assess their own uncertainty. Kadavath et al.~\cite{kadavath2022language} showed that models can estimate the probability of their answers being correct. Huang et al.~\cite{huang2024survey} provide a comprehensive survey of LLM calibration, highlighting the gap between model confidence and actual accuracy. Lin et al.~\cite{lin2024generating} study methods for eliciting confidence from LLMs. However, this work focuses on factual accuracy rather than safety decisions, leaving open the question of whether safety refusal behavior is similarly well-calibrated.

\textbf{Temperature Effects on Generation.} The temperature parameter controls the randomness of LLM sampling by scaling the logit distribution before sampling~\cite{holtzman2020curious}. Higher temperatures increase diversity but can reduce coherence. Renze and Guven~\cite{renze2024temperature} study how temperature affects problem-solving performance. While temperature effects on general generation quality are well-documented, no prior work has systematically quantified how temperature affects the consistency of safety-critical decisions.

\textbf{Red Teaming and Jailbreaking.} Automated red teaming techniques~\cite{perez2022red, ganguli2022red} attempt to find inputs that elicit harmful outputs from aligned models. Wei et al.~\cite{wei2024jailbroken} analyze failure modes of LLM safety training, identifying categories of prompts that reliably bypass safety measures. This work complements ours by identifying \textit{which} prompts fail, while we study \textit{how reliably} the model makes consistent decisions on a given prompt. The existence of ``borderline'' prompts that inconsistently trigger safety refusals suggests that adversaries could exploit stochastic sampling to bypass safety measures.

\textbf{Safety Training.} Modern LLMs are trained to be helpful, harmless, and honest through techniques like RLHF~\cite{ouyang2022training, bai2022training}. These methods optimize for human preferences but may not explicitly encourage consistent behavior across sampling configurations. Our findings suggest that safety training could benefit from explicitly penalizing inconsistent refusal behavior.

\section{Methodology}

\subsection{Experimental Design}

\textbf{Models.} We evaluate four instruction-tuned models from three distinct families: (1) Llama 3.1 8B Instruct~\cite{grattafiori2024llama}, Meta's open-weight model; (2) Qwen 2.5 7B Instruct, from Alibaba's model family; (3) Qwen 3 8B, representing the latest Qwen generation; and (4) Gemma 3 12B Instruct, Google's recent instruction-tuned model. This selection spans parameter counts from 7B to 12B and represents three major model families with distinct training approaches.

\textbf{Dataset.} We use 876 harmful prompts from the BeaverTails dataset~\cite{ji2023beavertails}, covering categories including violence, hate speech, illegal activities, and other harmful content.

\textbf{Sampling Configurations.} We test 4 temperature values (0.0, 0.3, 0.7, 1.0) $\times$ 5 random seeds (42--46), yielding 20 unique configurations per model and 70,080 total responses across all four models. At temperature 0.0, we use greedy decoding; any residual variation across seeds may arise from GPU non-determinism in floating-point operations. All inference was performed using vLLM~\cite{kwon2023vllm} for efficient batch generation.

\textbf{Response Classification.} We use Llama 3.1 70B Instruct as our primary judge model to classify each response into three categories. For the newer models (Qwen 3 and Gemma 3), we additionally employ Claude 3.5 Haiku as an external judge from a different model family to validate findings and address potential same-family bias concerns:
\begin{itemize}
    \item \textbf{REFUSE}: Model declines to answer or provides safety-focused redirection
    \item \textbf{PARTIAL}: Model provides limited information with caveats or warnings
    \item \textbf{COMPLY}: Model directly answers the harmful request
\end{itemize}

\subsection{Safety Stability Index (SSI)}

For a prompt $p$ with $N=20$ responses across sampling configurations, let $n_c$ denote the count of responses classified as category $c \in \{\text{REFUSE}, \text{PARTIAL}, \text{COMPLY}\}$. We define the Safety Stability Index as:

\begin{equation}
\text{SSI}(p) = \frac{\max_c(n_c)}{N}
\end{equation}

SSI ranges from $1/3$ (uniform distribution across all three categories) to $1.0$ (perfect agreement on a single decision). We define a prompt as \textit{unstable} if $\text{SSI}(p) < 0.8$, corresponding to fewer than 16 of 20 samples agreeing on the majority decision.

We define the per-prompt compliance rate as the fraction of responses classified as COMPLY:

\begin{equation}
\text{CR}(p) = \frac{n_{\text{COMPLY}}}{N}
\end{equation}

In our analyses, PARTIAL responses are treated as a distinct category, separate from both full refusal (REFUSE) and full compliance (COMPLY). This three-way classification allows us to distinguish between complete safety failures and responses that provide limited information with caveats.

We aggregate SSI scores across prompts to compute mean stability by temperature and identify the proportion of unstable prompts. We also track the \textit{flip rate}: the percentage of prompts that produce at least one different decision across configurations.

\section{Results}

\subsection{Overall Stability}

Figure~\ref{fig:flip_rate} shows that 68\% of prompts produce consistent safety decisions across all 20 configurations, while 32\% [95\% CI: 28.9\%--35.0\%] exhibit at least one flip between different decisions. Of these, 14.3\% [95\% CI: 12.0\%--16.6\%] show substantial instability (SSI $<$ 0.8), while the remaining 17.7\% have occasional flips but maintain strong majority agreement.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/flip_rate.pdf}
\caption{Distribution of consistent vs. inconsistent (flip) prompts across 876 harmful prompts tested on Llama 3.1 8B Instruct.}
\label{fig:flip_rate}
\end{figure}

The overall response distribution shows that the model predominantly refuses harmful requests (83.1\% REFUSE, 10.9\% PARTIAL, 5.9\% COMPLY), suggesting strong safety alignment on average. However, the substantial proportion of unstable prompts indicates that aggregate statistics obscure significant per-prompt variability.

\subsection{Distribution of Stability Scores}

Figure~\ref{fig:ssi_dist} presents the distribution of SSI scores across all 876 prompts. The distribution is heavily right-skewed with a strong mode at perfect stability (SSI = 1.0): the median SSI is 1.0, indicating that more than half of prompts produce identical responses across all configurations. However, 14.3\% of prompts fall below the 0.8 stability threshold, demonstrating that instability affects a meaningful fraction of safety evaluations.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/ssi_distribution.pdf}
\caption{Distribution of Safety Stability Index (SSI) scores. The dashed line at 0.8 indicates the threshold for unstable prompts. 14.3\% of prompts fall below this threshold.}
\label{fig:ssi_dist}
\end{figure}

\subsection{Temperature Effects}

Figure~\ref{fig:temp_effect} illustrates the relationship between temperature and both compliance rate and mean SSI. We observe that stability decreases monotonically as temperature increases, with this effect being highly significant ($\chi^2 = 133.15$, $p < 0.001$; Kruskal-Wallis $H = 185.43$, $p < 0.001$). Mean SSI drops from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. The flip rate more than doubles from 9.5\% at temperature 0.0 to 19.6\% at temperature 1.0.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/temperature_effect.pdf}
\caption{Effect of temperature on comply rate (blue) and mean Safety Stability Index (green). Higher temperatures reduce stability while showing non-monotonic effects on compliance.}
\label{fig:temp_effect}
\end{figure}

Interestingly, the lowest compliance rate occurs at temperature 1.0 (3.0\%). However, this likely reflects that high-temperature sampling produces less coherent outputs that fail to meaningfully engage with the harmful request, rather than representing more robust safety behavior.

\subsection{Stability vs. Compliance Analysis}

Figure~\ref{fig:stability_compliance} presents a scatter plot of per-prompt SSI against compliance rate. The distribution reveals two main regions:

\begin{itemize}
    \item \textbf{Stable Refusers} (left, high SSI): The majority of prompts cluster here with low compliance and high stability---the model consistently refuses these prompts
    \item \textbf{Variable Region} (center-right): A smaller set of prompts shows lower stability with varying compliance rates---these are the borderline cases where the model's decision is sensitive to sampling
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/stability_vs_compliance.pdf}
\caption{Safety Stability Index vs. compliance rate per prompt. Most prompts cluster in the stable-refusal region (upper left), while a subset shows variable behavior.}
\label{fig:stability_compliance}
\end{figure}

The existence of borderline prompts suggests that certain phrasings or topics create ambiguity in the model's safety classification, causing it to oscillate between refusing and complying depending on stochastic factors.

\subsection{What Predicts Prompt Instability?}

To understand what makes certain prompts unstable, we computed linguistic features for each prompt and analyzed their correlation with SSI. We find that prompt characteristics meaningfully predict instability:

\begin{itemize}
    \item \textbf{Sensitive terms increase stability}: Prompts containing obvious harmful keywords (e.g., ``hack,'' ``bomb,'' ``kill'') have \textit{higher} SSI ($r = 0.18$), suggesting the model confidently refuses clearly harmful requests.
    \item \textbf{Multi-sentence prompts are less stable}: Prompts with multiple sentences show lower SSI ($r = -0.21$), possibly because complex requests create ambiguity.
\end{itemize}

A logistic regression classifier predicting instability from linguistic features achieves AUC = 0.70 in 5-fold cross-validation, indicating that surface-level features partially explain which prompts will be unstable. The most predictive features are average word length, sensitive term count, and word count.

Qualitatively, the most unstable prompts (SSI $<$ 0.5) are predominantly \textbf{copyright-related requests} (e.g., requests for song lyrics or book passages) where the model oscillates between refusing, providing partial content with disclaimers, and complying. This suggests copyright represents a particularly ambiguous category in the model's safety training.

Table~\ref{tab:examples} provides examples of prompts exhibiting high instability, where the model's decision appears sensitive to stochastic sampling factors. These prompts demonstrate a range of instability patterns: some alternate primarily between REFUSE and PARTIAL, while others show more pronounced variation including compliance in some configurations.

\begin{table}[t]
\centering
\caption{Example prompts exhibiting decision instability across sampling configurations (N=20).}
\label{tab:examples}
\begin{tabular}{lcccc}
\toprule
\textbf{Prompt Category} & \textbf{SSI} & \textbf{REFUSE} & \textbf{PARTIAL} & \textbf{COMPLY} \\
\midrule
Request for copyrighted content & 0.55 & 2/20 & 11/20 & 7/20 \\
Request for harmful instructions & 0.45 & 3/20 & 9/20 & 8/20 \\
Request for copyrighted passage & 0.50 & 10/20 & 7/20 & 3/20 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model Comparison}

Figure~\ref{fig:model_comparison} compares stability across the four evaluated models. We observe consistent patterns of instability across all three model families, with instability rates (SSI $<$ 0.8) ranging from 14\% to 28\%.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/multimodel_stability.pdf}
\caption{Safety Stability Index comparison across models. Left: Mean SSI with standard deviation error bars. Right: Percentage of prompts with SSI $<$ 0.8 (unstable). All four models show instability rates between 6.7\% and 12.0\%.}
\label{fig:model_comparison}
\end{figure}

Table~\ref{tab:multimodel} summarizes the stability metrics across all four models from three families, using Claude 3.5 Haiku as a unified judge for methodological consistency. The models span a range of stability characteristics: Gemma 3 12B shows the highest stability (SSI=0.965, 18.4\% flip rate) while also having the lowest refusal rate (78.5\%), while the Qwen models show the lowest stability (SSI=0.938). Notably, Llama 3.1 8B falls in the middle (SSI=0.944, 27.3\% flip rate). All models exhibit non-trivial instability (6.7--12.0\% of prompts highly unstable) despite different training approaches and model families, confirming that safety decision instability is a fundamental property of current alignment approaches rather than a model-specific artifact.

\begin{table}[h]
\centering
\caption{Stability comparison across model families. All models evaluated using Claude 3.5 Haiku as judge for methodological consistency. Models sorted by SSI (descending).}
\label{tab:multimodel}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Mean SSI} $\uparrow$ & \textbf{Flip Rate} & \textbf{\% Unstable} & \textbf{Refusal Rate} \\
\midrule
Gemma-3-12B-Instruct & \textbf{0.965} & 18.4\% & \textbf{6.7\%} & 78.5\% \\
Llama-3.1-8B-Instruct & 0.944 & 27.3\% & 10.4\% & 79.3\% \\
Qwen3-8B & 0.938 & 27.7\% & 11.8\% & 92.5\% \\
Qwen2.5-7B-Instruct & 0.938 & 26.3\% & 12.0\% & 81.3\% \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:model_distribution} shows the response distribution (REFUSE, PARTIAL, COMPLY) for each model. The models differ in their base refusal rates: Qwen is more conservative (90.8\% refuse) while Llama shows a slightly lower refusal rate (83.1\%) with more PARTIAL responses (10.9\% vs 3.9\%).

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/multimodel_distribution.pdf}
\caption{Response distribution by model. Green indicates refusal, orange partial compliance, and red full compliance.}
\label{fig:model_distribution}
\end{figure}

\textbf{Judge Model.} We use Llama 3.1 70B Instruct as the primary judge model, which is distinct from the 8B models being evaluated. Prior work on LLM-as-judge has shown strong correlation with human annotations for safety-related tasks~\cite{zheng2023judging}. The three-class rubric (REFUSE, PARTIAL, COMPLY) was designed to capture the spectrum of safety responses, with PARTIAL indicating hedged or caveat-laden responses that neither fully refuse nor fully comply.

\subsection{External Judge Validation}

To address potential same-family bias concerns with our Llama-based judge and ensure methodological consistency across all models, we evaluated \textit{all} 70,080 responses using Claude 3.5 Haiku as an external judge from Anthropic's model family. This provides cross-validation with a judge trained on entirely different data and by a different organization.

\textbf{Inter-Judge Agreement.} Comparing Claude 3.5 Haiku to Llama 70B judgments on the Llama 3.1 8B and Qwen 2.5 7B responses (35,040 samples), we observe 91.7\% exact agreement on the three-class classification, with Cohen's $\kappa = 0.744$ indicating substantial inter-rater reliability. Claude Haiku consistently assigns slightly higher SSI scores (mean difference: +0.05), suggesting marginally stricter refusal detection. Crucially, the \textit{relative ranking} of model stability remains consistent across judges.

\textbf{Unified Results.} Table~\ref{tab:multimodel} reports all metrics using Claude 3.5 Haiku as the judge for all four models, ensuring methodological consistency in cross-model comparisons. The high inter-judge agreement (91.7\%, $\kappa = 0.744$) validates that our instability findings are robust to judge selection and not artifacts of same-family bias.

\subsection{Sample Size Requirements for Reliable Evaluation}

A key practical question is: how many samples are needed for reliable safety evaluation? We address this by simulating subsampling from our N=20 ground truth. For each prompt, we randomly draw $n$ samples (where $n \in \{1, 2, 3, ..., 20\}$), compute the majority label, and measure agreement with the full-data majority.

Figure~\ref{fig:sample_size} shows that single-shot evaluation (N=1) agrees with the ground truth only 92.4\% [95\% CI: 91.0\%--93.8\%] of the time---meaning approximately 7.6\% of prompts would be misclassified by single-sample testing. Agreement rises to 95\% at N=3 and reaches 99\% only at N=20.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/sample_size_reliability.pdf}
\caption{Evaluation reliability as a function of sample size. Single-shot testing (N=1) achieves only 92.4\% agreement with ground truth. The 95\% reliability threshold requires N$\geq$3 samples.}
\label{fig:sample_size}
\end{figure}

\begin{table}[h]
\centering
\caption{Sample size requirements for reliable safety evaluation.}
\label{tab:sample_size}
\begin{tabular}{ccc}
\toprule
\textbf{N Samples} & \textbf{Agreement (\%)}& \textbf{95\% CI} \\
\midrule
1 & 92.4 & [91.0, 93.8] \\
3 & 95.0 & [93.8, 96.2] \\
5 & 96.3 & [95.1, 97.3] \\
10 & 97.8 & [96.7, 98.5] \\
20 & 99.4 & [99.1, 99.8] \\
\bottomrule
\end{tabular}
\end{table}

These results suggest that safety benchmarks should use at least 3 samples per prompt for 95\% reliability, with more samples recommended for high-stakes deployment decisions.

\section{Discussion}

\subsection{Implications for Safety Evaluation}

Our findings demonstrate that single-shot safety evaluations can significantly misrepresent model safety. A prompt classified as ``safe'' in one evaluation may produce a harmful response under different sampling conditions. This variability has several important implications:

\textbf{Benchmark Reliability.} Safety benchmarks that report only aggregate pass rates may be highly sensitive to random seed choice. Different evaluators using different random seeds could reach contradictory conclusions about the same model.

\textbf{Deployment Considerations.} For safety-critical applications, using deterministic sampling (temperature 0.0) provides the most stable behavior (mean SSI = 0.951). At this temperature, only 9.5\% of prompts show instability, compared to 19.6\% at temperature 1.0.

\textbf{Adversarial Robustness.} The existence of borderline prompts that flip between refuse and comply suggests that adversaries could exploit stochastic sampling by repeatedly querying the model until they obtain a harmful response.

\textbf{Evaluation Protocol Recommendations.} Our sample size analysis provides concrete guidance: single-shot evaluation misclassifies 7.6\% of prompts. For benchmark reliability, we recommend a minimum of N=3 samples per prompt (95\% agreement). For deployment decisions, N=10+ samples provide 98\% reliability. When computational constraints require single-shot evaluation, results should be reported with appropriate uncertainty bounds acknowledging the $\sim$8\% error rate.

\subsection{Limitations}

\textbf{Model Scale.} While we evaluate four models from three families (Llama, Qwen, and Gemma), all are in the 7-12B parameter range. Larger models (70B, 405B) or closed-source models (GPT-4, Claude) may exhibit different stability characteristics. Future work should extend this analysis across model scales.

\textbf{Judge Model.} We primarily use Llama 3.1 70B Instruct as the judge, supplemented by Claude 3.5 Haiku for cross-validation of newer models. While the consistency between these judges increases confidence in our findings, human annotation on a subset of responses would further strengthen conclusions.

\textbf{Limited Seed Count.} We test only 5 seeds per temperature. While sufficient to detect instability, more seeds would provide tighter confidence intervals on stability estimates.

\textbf{Dataset Scope.} Our analysis uses BeaverTails prompts. Different prompt distributions (e.g., adversarially-crafted jailbreaks from HarmBench) may exhibit different stability patterns.

\textbf{Binary Stability Threshold.} Our SSI $<$ 0.8 threshold for instability is somewhat arbitrary. Practitioners with different risk tolerances may prefer stricter or more lenient thresholds.

\section{Conclusion}

We have demonstrated that safety refusal decisions in LLMs exhibit significant instability across random seeds and temperature settings. Evaluating four instruction-tuned models from three families (Llama, Qwen, and Gemma) across 70,080 total responses, we find that 18--28\% of prompts exhibit decision flips depending on the model. Temperature significantly affects stability ($p < 0.001$), with instability rates increasing from 9.5\% at temperature 0.0 to 19.6\% at temperature 1.0. Importantly, these patterns are consistent across all model families and validated by an external Claude judge, suggesting that safety decision instability is a fundamental property of current alignment approaches rather than a model-specific artifact.

Critically, single-shot evaluation---the standard practice in safety benchmarking---agrees with multi-sample ground truth only 92.4\% of the time. This finding challenges the validity of single-shot safety evaluations and suggests that current benchmarks may provide incomplete assessments of model safety.

We recommend that safety evaluation protocols: (1) use at least N=3 samples per prompt for 95\% reliability, (2) report stability metrics alongside aggregate pass rates, (3) use lower temperatures for safety-critical deployments, and (4) implement ensemble voting for high-stakes decisions. Future work should explore whether larger models (70B+) or different safety training techniques (DPO, constitutional AI) produce more stable behavior.

The code and data for this study are available at \url{https://github.com/erikl2/safety-refusal-stability}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
