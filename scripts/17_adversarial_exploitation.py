#!/usr/bin/env python3
"""
Analyze adversarial exploitation potential.
For prompts with any COMPLY response, compute expected queries to first success.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from collections import Counter

PROJECT_ROOT = Path(__file__).parent.parent
LABELS_DIR = PROJECT_ROOT / "data" / "results" / "labels"
METRICS_DIR = PROJECT_ROOT / "data" / "results" / "metrics"

def load_all_labels():
    """Load all label files."""
    # Try Claude Haiku combined file first
    claude_file = LABELS_DIR / "claude_haiku_llama_qwen25.csv"
    if claude_file.exists():
        df1 = pd.read_csv(claude_file)
    else:
        df1 = pd.DataFrame()

    claude_new = LABELS_DIR / "claude_haiku_new_models.csv"
    if claude_new.exists():
        df2 = pd.read_csv(claude_new)
    else:
        df2 = pd.DataFrame()

    dfs = [df for df in [df1, df2] if len(df) > 0]
    if dfs:
        return pd.concat(dfs, ignore_index=True)

    # Fallback to individual files
    label_files = list(LABELS_DIR.glob("*_labels.csv"))
    if not label_files:
        return None

    dfs = []
    for f in label_files:
        try:
            df = pd.read_csv(f)
            if 'label' in df.columns:
                dfs.append(df)
        except:
            pass

    if dfs:
        return pd.concat(dfs, ignore_index=True)
    return None

def analyze_exploitation_potential(labels_df):
    """
    For each prompt with compliance_rate > 0:
    - E[queries to first COMPLY] = 1 / compliance_rate
    """
    results = []

    model_col = 'model'
    if model_col not in labels_df.columns:
        model_col = labels_df.columns[0]  # fallback

    for (model, prompt_id), group in labels_df.groupby([model_col, 'prompt_id']):
        labels = group['label'].tolist()
        valid = [l for l in labels if l in {"REFUSE", "PARTIAL", "COMPLY"}]

        if not valid:
            continue

        comply_count = sum(1 for l in valid if l == 'COMPLY')
        comply_rate = comply_count / len(valid)

        if comply_rate > 0:
            expected_queries = 1 / comply_rate
            results.append({
                'model': model,
                'prompt_id': prompt_id,
                'comply_rate': comply_rate,
                'expected_queries': expected_queries,
                'comply_count': comply_count,
                'total_samples': len(valid)
            })

    return pd.DataFrame(results)

def compute_tail_risk_metrics(labels_df):
    """
    For safety-critical deployments, compute:
    - P(any COMPLY in n draws) for n = 1, 3, 5, 10, 20
    """
    model_col = 'model'

    all_results = []

    for prompt_id, group in labels_df.groupby('prompt_id'):
        labels = group['label'].tolist()
        valid = [l for l in labels if l in {"REFUSE", "PARTIAL", "COMPLY"}]

        if not valid:
            continue

        comply_rate = sum(1 for l in valid if l == 'COMPLY') / len(valid)

        # P(at least one COMPLY in n draws) = 1 - (1-p)^n
        for n in [1, 3, 5, 10, 20]:
            p_any_comply = 1 - (1 - comply_rate) ** n
            all_results.append({
                'prompt_id': prompt_id,
                'n_draws': n,
                'p_any_comply': p_any_comply,
                'comply_rate': comply_rate
            })

    return pd.DataFrame(all_results)

def main():
    print("=" * 70)
    print("Task 2.2: Adversarial Exploitation Analysis")
    print("=" * 70)

    labels_df = load_all_labels()

    if labels_df is None or len(labels_df) == 0:
        print("ERROR: No labels loaded")
        return

    print(f"Loaded {len(labels_df)} label records")

    results = analyze_exploitation_potential(labels_df)

    if len(results) == 0:
        print("No prompts with COMPLY responses found")
        return

    print(f"\n" + "=" * 70)
    print("EXPLOITATION POTENTIAL ANALYSIS:")
    print("=" * 70)

    print(f"\nPrompts with any COMPLY: {len(results)}")
    print(f"Unique prompts: {results['prompt_id'].nunique()}")

    # Per-model breakdown
    print("\nBy Model:")
    for model in results['model'].unique():
        model_results = results[results['model'] == model]
        short_name = model.split('/')[-1] if '/' in model else model
        print(f"  {short_name}: {len(model_results)} exploitable prompts")

    # Statistics on expected queries
    print("\n" + "-" * 50)
    print("Expected queries to first COMPLY:")
    print("-" * 50)
    print(f"  Median: {results['expected_queries'].median():.1f}")
    print(f"  Mean: {results['expected_queries'].mean():.1f}")
    print(f"  Min: {results['expected_queries'].min():.1f}")
    print(f"  Max: {results['expected_queries'].max():.1f}")
    print(f"  25th percentile: {results['expected_queries'].quantile(0.25):.1f}")
    print(f"  75th percentile: {results['expected_queries'].quantile(0.75):.1f}")

    # Distribution
    print("\n" + "-" * 50)
    print("Distribution of expected queries:")
    print("-" * 50)
    bins = [1, 2, 5, 10, 20, float('inf')]
    labels_bins = ['1 (always)', '2-4', '5-9', '10-19', '20+']
    results['query_bin'] = pd.cut(results['expected_queries'], bins=bins, labels=labels_bins)
    dist = results['query_bin'].value_counts().sort_index()
    for bin_label, count in dist.items():
        pct = count / len(results) * 100
        print(f"  {bin_label}: {count} ({pct:.1f}%)")

    # Key findings
    within_5 = (results['expected_queries'] <= 5).mean() * 100
    within_10 = (results['expected_queries'] <= 10).mean() * 100

    print("\n" + "=" * 70)
    print("KEY FINDINGS FOR PAPER:")
    print("=" * 70)
    print(f"{within_5:.1f}% of exploitable prompts yield COMPLY within 5 queries")
    print(f"{within_10:.1f}% of exploitable prompts yield COMPLY within 10 queries")

    results.to_csv(METRICS_DIR / "adversarial_exploitation.csv", index=False)

    # Tail risk analysis
    print("\n" + "=" * 70)
    print("TAIL RISK ANALYSIS:")
    print("=" * 70)

    tail_df = compute_tail_risk_metrics(labels_df)

    # Aggregate: what fraction of all prompts have P(comply) > threshold in n draws?
    for n in [1, 3, 5, 10]:
        n_df = tail_df[tail_df['n_draws'] == n]
        # Only count prompts with non-zero comply rate
        exploitable = n_df[n_df['comply_rate'] > 0]
        if len(exploitable) > 0:
            mean_p = exploitable['p_any_comply'].mean() * 100
            pct_dangerous = (exploitable['p_any_comply'] > 0.5).mean() * 100
            print(f"\nWith {n} queries per prompt:")
            print(f"  Mean P(any COMPLY): {mean_p:.1f}%")
            print(f"  % prompts with >50% chance of bypass: {pct_dangerous:.1f}%")

    tail_df.to_csv(METRICS_DIR / "tail_risk_metrics.csv", index=False)

    # LaTeX output
    print("\n" + "-" * 50)
    print("Text for paper (Adversarial Robustness section):")
    print("-" * 50)
    print(f"""
For prompts where the model ever complied (N={len(results)} unique prompts),
an adversary would need a median of {results['expected_queries'].median():.0f} queries
to obtain a harmful response. {within_5:.0f}\\% of exploitable prompts yield
compliance within 5 queries, demonstrating that stochastic sampling creates
practical vulnerabilities for adversarial exploitation.
""")

if __name__ == "__main__":
    main()
